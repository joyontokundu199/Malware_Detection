#!/usr/bin/env python
# coding: utf-8

# # Malware Detection Using Machine Learning And Deep Learning

# ## Importing Libraires

# In[26]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


# ## Exploring The Malware Data Set

# In[47]:


malware_data=pd.read_csv("C:/Users/rajak/OneDrive/Desktop/Malware/\MalwareData.csv",sep="|",low_memory='TRUE')
malware_data


# In[48]:


malware_data.head()


# In[49]:


malware_data.shape


# In[50]:


malware_data.describe()


# In[51]:


legitimate=malware_data[0:41323].drop(["legitimate"],axis=1)
malware=malware_data[41323::].drop(["legitimate"],axis=1)
print("The Shape Of Legitimate Dataset is %s Samples,%s Features"%(legitimate.shape[0],legitimate.shape[1]))
print("The Shape Of malware Dataset is %s Samples,%s Features"%(malware.shape[0],malware.shape[1]))     


# In[52]:


fig=plt.figure()
ax=fig.add_axes([0,0,1,1])
ax.hist(malware_data['legitimate'],20)
plt.show()


# ## DATA CLEANING

# In[53]:


y=malware_data['legitimate']
malware_data=malware_data.drop(['legitimate'],axis=1)


# In[54]:


malware_data=malware_data.drop(['Name'],axis=1)
malware_data=malware_data.drop(['md5'],axis=1)
print("  The Name and md5 variables are removed successfully")


# In[55]:


malware_data


# ## Spliting The Dataset Into Test and Train

# In[56]:


from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(malware_data,y,test_size=0.2,random_state=42)


# In[57]:


x_train.shape


# ## MODEL BUILDING

# ### 1- Random Forest

# ### Random forest is a Supervised Machine Learning Algorithm that is used widely in Classification and Regression problems. It builds decision trees on different samples and takes their majority vote for classification and average in case of regression.
# 
# ### One of the most important features of the Random Forest Algorithm is that it can handle the data set containing continuous variables as in the case of regression and categorical variables as in the case of classification. It performs better results for classification problems.

# ![Screenshot%20%28535%29.png](attachment:Screenshot%20%28535%29.png)

# In[58]:


y_train


# In[59]:


x_train


# In[60]:


from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
clf=RandomForestClassifier(max_depth=2,random_state=0)
randomModel=clf.fit(x_train,y_train)


# ## Random Forest Evaluation On Test Data

# In[61]:


from sklearn.metrics import f1_score,accuracy_score,plot_confusion_matrix,auc,confusion_matrix


# In[62]:


# Accuracy on the train dataset
train_pred=randomModel.predict(x_train)
accuracy_score(y_train,train_pred)


# In[63]:


# Accuracy on the test dataset 
prediction=randomModel.predict(x_test)
accuracy_score(y_train,train_pred)


# In[64]:


f1_score(y_test,prediction)


# ##  Confusion Matrix
# 
# 
# ### A confusion matrix is a summary of prediction results on a classification problem.
# 
# ### The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix.
# 
# ### The confusion matrix shows the ways in which your classification model
# ### is confused when it makes predictions.
# 
# ### It gives you insight not only into the errors being made by your classifier but more importantly the types of errors that are being made.
# 
# ### It is this breakdown that overcomes the limitation of using classification accuracy alone.
# 
# 

# ![Screenshot%20%28534%29.png](attachment:Screenshot%20%28534%29.png)

# In[107]:


titles_options=[("Confusion Matrix,Without Normalization",None),
               ("Normalized Confusion Matrix",'true')]
for title,normalize in titles_options:
    disp=plot_confusion_matrix(randomModel,x_test,y_test,
                               
                               cmap=plt.cm.Blues,
                               normalize=normalize)
    disp.ax_.set_title(title)
    
    print(title)
    print(disp.confusion_matrix)
    
    plt.show()


# ## 2- Logistic Regression

# ### Logistic regression predicts the probability of an outcome that can only have two values (i.e. a dichotomy). The prediction is based on the use of one or several predictors (numerical and categorical). A linear regression is not appropriate for predicting the value of a binary variable for two reasons:		
# 
# ### A linear regression will predict values outside the acceptable range (e.g. predicting probabilities
# ### outside the range 0 to 1)
# ### Since the dichotomous experiments can only have one of two possible values for each experiment, the residuals will not be normally distributed about the predicted line.

# ![Screenshot%20%28536%29.png](attachment:Screenshot%20%28536%29.png)

# ![Screenshot%20%28537%29.png](attachment:Screenshot%20%28537%29.png)

# In[78]:


from sklearn.linear_model import LogisticRegression

clf =LogisticRegression (random_state=0)
logModel=clf.fit(x_train, y_train)


# ## Model Evaluation
# 

# In[79]:


# Accuracy on the train dataset
train_log=logModel.predict(x_train)
accuracy_score(y_train,train_log)


# In[80]:


# Accuracy on the test dataset
pred=logModel.predict(x_test)
accuracy_score(y_test,pred)


# In[81]:


f1_score(y_test,pred)


# In[85]:


titles_options=[("Confusion Matrix,Without Normalization",None),
               ("Normalized Confusion Matrix",'true')]
for title,normalize in titles_options:
    disp=plot_confusion_matrix(logModel,x_test,y_test,
                                                       cmap=plt.cm.Blues,
                               normalize=normalize)
    disp.ax_.set_title(title)
    
    print(title)
    print(disp.confusion_matrix)
    
    plt.show()


# ## 3- NEURAL NETWORK 

# ### A neural network is a method in artificial intelligence that teaches computers to process data in a way that is inspired by the human brain. It is a type of machine learning process, called deep learning, that uses interconnected nodes or neurons in a layered structure that resembles the human brain.

# ## Why are neural networks important?

# ### Neural networks can help computers make intelligent decisions with limited human assistance. This is because they can learn and model the relationships between input and output data that are nonlinear and complex. For instance, they can do the following tasks.

# In[88]:


get_ipython().system('pip install tensorflow')


# ## TENSORFLOW:

# ### TensorFlow, which competes with frameworks such as PyTorch and Apache MXNet, can train and run deep neural networks for handwritten digit classification, image recognition, word embeddings, recurrent neural networks, sequence-to-sequence models for machine translation, natural language processing, and PDE (partial differential equation)-based simulations. Best of all, TensorFlow supports production prediction at scale, with the same models used for training.

# In[89]:


import tensorflow as tf

from tensorflow.keras.models import Sequential

from tensorflow.keras.layers import Dense


# In[93]:


# Define model

model=Sequential()

model.add(Dense (16, input_dim=54, activation ="relu")) 
model.add(Dense (8, activation= "relu"))

model.add(Dense (4, activation= "relu"))

model.add(Dense (1, activation ='sigmoid'))

model.summary() #Print model Summary


# In[95]:


#Compile Model
model.compile(loss ="binary_crossentropy", optimizer="rmsprop", metrics=["accuracy"])


# In[98]:


#Fit Model
model.fit(x_train, y_train, epochs =5, batch_size=32)


# ## Model Evaluation

# In[99]:


# Accuracy on the training dataset
trainPred=model.predict(x_train)
trainPred=[1 if y>=0.5 else 0 for y in trainPred]
accuracy_score(y_train,trainPred)


# In[103]:


# Accuracy on the test dataset
y_prediction=model.predict(x_test)
y_prediction=[1 if y>=0.5 else 0   for y in y_prediction]
accuracy_score(y_test,y_prediction)


# In[104]:


confusion_matrix(y_test,y_prediction)


# In[105]:


f1_score(y_test,y_prediction)


# In[ ]:




